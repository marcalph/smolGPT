{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompting techniques w/ llama3.2\n",
    "\n",
    "requirements:\n",
    "```\n",
    "> poetry run pip install dspy\n",
    "> curl -fsSL https://ollama.ai/install.sh | sh\n",
    "> ollama run llama3.2:1b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "lm = dspy.LM('ollama_chat/llama3.2', api_base='http://localhost:11434', api_key='')\n",
    "mistral = dspy.LM(lm=lm)\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"When I was 6 my sister was half my age. Now, Iâ€™m 70 how old is my sister?\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Zero-shot Prompting\n",
    "- Few-shot Prompting\n",
    "- Chain-of-Thought Prompting\n",
    "- Meta Prompting\n",
    "- Self-Consistency\n",
    "- Generate Knowledge Prompting\n",
    "- Prompt Chaining\n",
    "- Tree of Thoughts\n",
    "- Retrieval Augmented Generation\n",
    "- Automatic Reasoning and Tool-use\n",
    "- Automatic Prompt Engineer\n",
    "- Active-Prompt\n",
    "- Directional Stimulus Prompting\n",
    "- Program-Aided Language Models\n",
    "- ReAct\n",
    "- Reflexion\n",
    "- Multimodal CoT\n",
    "- Graph Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import outlines\n",
    "\n",
    "# model = outlines.models.mlxlm(\"mlx-community/Phi-3-mini-4k-instruct-4bit\")\n",
    "model = outlines.models.llamacpp(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct-gguf\", \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = outlines.generate.text(model)\n",
    "\n",
    "result = generator(\"Question: What's 2+2? Answer:\", max_tokens=20)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from pydantic import BaseModel, constr, conint\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: constr(max_length=10)\n",
    "    age: conint(gt=18, lt=99)\n",
    "    armor: (Enum('Armor', {'leather': 'leather', 'chainmail': 'chainmail', 'plate': 'plate'}))\n",
    "    strength: conint(gt=1, lt=100)\n",
    "\n",
    "generator = outlines.generate.json(model, Character)\n",
    "\n",
    "character = generator(\n",
    "    \"Generate a new character for my awesome game: \"\n",
    "    + \"name, age (between 18 and 99), armor and strength. \"\n",
    "    )\n",
    "print(character)\n",
    "# Character(name='Zara', age=25, armor=<Armor.leather: 'leather'>, strength=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smolgpt-LttoHTux-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
